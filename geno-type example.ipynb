{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea12c45",
   "metadata": {},
   "source": [
    "Example of simple network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8de61917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=16, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "def create_neural_network(input_dims, output_dims, hidden_dims=[64, 32], activation=nn.ReLU()):\n",
    "    layers = []\n",
    "    \n",
    "    # Add input layer\n",
    "    layers.append(nn.Linear(input_dims, hidden_dims[0]))\n",
    "    layers.append(activation)\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for i in range(1, len(hidden_dims)):\n",
    "        layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "        layers.append(activation)\n",
    "    \n",
    "    # Add output layer\n",
    "    layers.append(nn.Linear(hidden_dims[-1], output_dims))\n",
    "    \n",
    "    # Create the neural network\n",
    "    model = nn.Sequential(*layers)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_dims = 10\n",
    "output_dims = 5\n",
    "hidden_dims = [64, 32, 16]  # You can adjust the number of hidden layers and their sizes\n",
    "activation = nn.ReLU()\n",
    "\n",
    "net = create_neural_network(input_dims, output_dims, hidden_dims, activation)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06484342",
   "metadata": {},
   "source": [
    "Creating complex network with residual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "577c18a0",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplexNeuralNetwork(\n",
      "  (network): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): Flatten(start_dim=1, end_dim=-1)\n",
      "    (6): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=32, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.stride != 1 or identity.shape[1] != out.shape[1]:\n",
    "            identity = nn.Conv2d(identity.shape[1], out.shape[1], kernel_size=1, stride=self.stride, bias=False)(identity)\n",
    "            identity = nn.BatchNorm2d(out.shape[1])(identity)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ComplexNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dims=1, output_dims=10, conv_channels=[16, 32], hidden_dims=[64, 32], activation=nn.ReLU()):\n",
    "        super(ComplexNeuralNetwork, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Initial convolutional layer\n",
    "        layers.append(nn.Conv2d(in_channels=input_dims, out_channels=conv_channels[0], kernel_size=3, padding=1))\n",
    "        layers.append(nn.BatchNorm2d(conv_channels[0]))\n",
    "        layers.append(activation)\n",
    "        \n",
    "        # Convolutional layers with skip connections (using ResidualBlocks)\n",
    "        in_channels = conv_channels[0]\n",
    "        for out_channels in conv_channels:\n",
    "            layers.append(ResidualBlock(in_channels, out_channels))\n",
    "            in_channels = out_channels\n",
    "        \n",
    "        # Flatten the output of the convolutional layers\n",
    "        layers.append(nn.Flatten())\n",
    "        \n",
    "        # Linear layers with skip connections\n",
    "        in_features = in_channels\n",
    "        for out_features in hidden_dims:\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            layers.append(nn.BatchNorm1d(out_features))\n",
    "            layers.append(activation)\n",
    "            in_features = out_features\n",
    "        \n",
    "        # Output linear layer\n",
    "        layers.append(nn.Linear(in_features, output_dims))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Example usage\n",
    "input_dims = 1  # Number of input channels (e.g., for RGB images)\n",
    "output_dims = 10  # Number of output classes\n",
    "conv_channels = [16, 32]  # Number of channels in convolutional layers\n",
    "hidden_dims = [64, 32]  # Sizes of hidden linear layers\n",
    "activation = nn.ReLU()\n",
    "\n",
    "net = ComplexNeuralNetwork(input_dims, output_dims, conv_channels, hidden_dims, activation)\n",
    "\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "31097d33-650a-473d-ae4c-ae2fef19cec9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x131072 and 32x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[95], line 67\u001b[0m, in \u001b[0;36mComplexNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x131072 and 32x64)"
     ]
    }
   ],
   "source": [
    "net(torch.rand(1,1,64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4094c43",
   "metadata": {},
   "source": [
    "Downloading the training and validation dataset cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "52c31dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "test_dataset = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5fce44c-f205-412b-a584-9f1f0b0625b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "853f7c84-6006-4ab5-af50-1c11d0cc89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "14c2313e-5e3b-451c-aaa1-c5cab984532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x25088 and 32x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Get the predictions\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 9\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(predictions, labels)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[90], line 67\u001b[0m, in \u001b[0;36mComplexNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x25088 and 32x64)"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        # Get the predictions\n",
    "        print(images.shape)\n",
    "        predictions = model(images)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: {} Loss: {:.4f}'.format(epoch, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54f03d-3637-4ba3-a161-86a261f50a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    predictions = model(images)\n",
    "    _, predicted = torch.max(predictions, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Test accuracy: {}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "85670158-2490-4926-afc2-b98249effb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=next(iter(train_dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c79093ab-5f3a-4b98-9e74-e33b422b1bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=torch.rand(64, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ac200f0-fa7d-4a45-ab1b-2409d9f006f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16, 28, 28])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer=nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "layer(t).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f35e480",
   "metadata": {},
   "source": [
    "Specifing the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "979362b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-03 13:53:38] INFO     < 1180> evotorch.core: Instance of `SupervisedNE` (id:1264978372240) -- The `dtype` for the problem's decision variables is set as torch.float32\n",
      "[2023-09-03 13:53:38] INFO     < 1180> evotorch.core: Instance of `SupervisedNE` (id:1264978372240) -- `eval_dtype` (the dtype of the fitnesses and evaluation data) is set as torch.float32\n",
      "[2023-09-03 13:53:38] INFO     < 1180> evotorch.core: Instance of `SupervisedNE` (id:1264978372240) -- The `device` of the problem is set as cpu\n",
      "[2023-09-03 13:53:38] INFO     < 1180> evotorch.core: Instance of `SupervisedNE` (id:1264978372240) -- The number of actors that will be allocated for parallelized evaluation is 4\n",
      "[2023-09-03 13:53:38] INFO     < 1180> evotorch.core: Instance of `SupervisedNE` (id:1264978372240) -- Number of GPUs that will be allocated per actor is None\n"
     ]
    }
   ],
   "source": [
    "from evotorch.neuroevolution import SupervisedNE\n",
    "\n",
    "mnist_problem = SupervisedNE(\n",
    "    train_dataset,  # Using the dataset specified earlier\n",
    "    net,  # Training the MNIST30K module designed earlier\n",
    "    nn.CrossEntropyLoss(),  # Minimizing CrossEntropyLoss\n",
    "    minibatch_size = 10,  # With a minibatch size of 256\n",
    "    common_minibatch = True,  # Always using the same minibatch across all solutions on an actor\n",
    "    num_actors = 4,  # The total number of CPUs used\n",
    "    num_gpus_per_actor = 'max',  # Dividing all available GPUs between the 4 actors\n",
    "    subbatch_size = 50,  # Evaluating solutions in sub-batches of size 50 ensures we won't run out of GPU memory for individual workers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "301e65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evotorch.algorithms import SNES\n",
    "searcher = SNES(mnist_problem, stdev_init = 1, popsize = 1000, distributed = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a24008c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evotorch.logging import StdOutLogger, PandasLogger\n",
    "stdout_logger = StdOutLogger(searcher, interval = 1)\n",
    "pandas_logger = PandasLogger(searcher, interval = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2623dbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(EvaluationActor pid=19748)\u001b[0m C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py:3425: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "\u001b[2m\u001b[36m(EvaluationActor pid=19748)\u001b[0m   shares_storage = self._data.storage().data_ptr() == source._data.storage().data_ptr()\n"
     ]
    },
    {
     "ename": "RayTaskError(RuntimeError)",
     "evalue": "\u001b[36mray::EvaluationActor.call()\u001b[39m (pid=19748, ip=127.0.0.1, actor_id=d5880e87f71e3599ee0a9bc201000000, repr=<evotorch.core.EvaluationActor object at 0x0000025037736A90>)\n  File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n    return method(self, *_args, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 185, in call\n    return getattr(self._problem, method_name)(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 3074, in _sample_and_compute_gradients\n    resulting_batch = sample_evaluated_batch()\n                      ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 3067, in sample_evaluated_batch\n    self.evaluate(batch)\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2395, in evaluate\n    self._evaluate_all(batch)\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2413, in _evaluate_all\n    self._evaluate_batch(batch)\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 345, in _evaluate_batch\n    return super()._evaluate_batch(batch)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2447, in _evaluate_batch\n    self._evaluate(sln)\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\neproblem.py\", line 424, in _evaluate\n    fitnesses = evaluator(self.parameterize_net(parameters))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 338, in _evaluate_network\n    loss += self._evaluate_using_minibatch(network, self._current_minibatch) / self._num_minibatches\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 264, in _evaluate_using_minibatch\n    yhat = network(x)\n           ^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_1180\\556631100.py\", line 67, in forward\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 217, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_1180\\556631100.py\", line 25, in forward\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRayTaskError(RuntimeError)\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msearcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\algorithms\\searchalgorithm.py:425\u001b[0m, in \u001b[0;36mSearchAlgorithm.run\u001b[1;34m(self, num_generations, reset_first_step_datetime)\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_first_step_datetime()\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(num_generations)):\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_end_of_run_hook) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_end_of_run_hook(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\algorithms\\searchalgorithm.py:390\u001b[0m, in \u001b[0;36mSearchAlgorithm.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_step_datetime \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_step_datetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_status({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_count})\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\algorithms\\distributed\\gaussian.py:202\u001b[0m, in \u001b[0;36mGaussianSearchAlgorithm._step_distributed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step_distributed\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# Use the problem object's `sample_and_compute_gradients` method\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# to do parallelized and distributed gradient computation\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m     fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproblem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_and_compute_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpopsize_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popsize_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obj_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_interactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_interactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mranking_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ranking_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_even_popsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_even_popsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# The method `sample_and_compute_gradients(...)` returns a list of dictionaries, each dictionary being\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# the result of a different remote computation.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# For each remote computation, the list will contain a dictionary that looks like this:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# mean_eval.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# These lists will be stored by the following temporary class:\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mlist_of\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py:2797\u001b[0m, in \u001b[0;36mProblem.sample_and_compute_gradients\u001b[1;34m(self, distribution, popsize, num_interactions, popsize_max, obj_index, ranking_method, with_stats, ensure_even_popsize)\u001b[0m\n\u001b[0;32m   2794\u001b[0m dist_on_cpu \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2796\u001b[0m \u001b[38;5;66;03m# Here, we use our actor pool to execute our tasks in parallel.\u001b[39;00m\n\u001b[1;32m-> 2797\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2798\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_actor_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2799\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2800\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2801\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_sample_and_compute_gradients\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2802\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdist_on_cpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2803\u001b[0m \u001b[43m                \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m   2804\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobj_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2805\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_interactions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2806\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpopsize_max\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2807\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mranking_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mranking_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2808\u001b[0m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2809\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2810\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpopsize_per_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_inter_per_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpopsize_max_per_task\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2812\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2815\u001b[0m \u001b[38;5;66;03m# At this point, all the tensors within our collected results are on the CPU.\u001b[39;00m\n\u001b[0;32m   2817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2818\u001b[0m     \u001b[38;5;66;03m# If the main device of this problem instance is not CPU, then we move the tensors to the main device.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\util\\actor_pool.py:161\u001b[0m, in \u001b[0;36mActorPool.map_unordered.<locals>.get_generator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_generator\u001b[39m():\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_next():\n\u001b[1;32m--> 161\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\util\\actor_pool.py:361\u001b[0m, in \u001b[0;36mActorPool.get_next_unordered\u001b[1;34m(self, timeout, ignore_if_timedout)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_timeout_after_ignore:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[0;32m    359\u001b[0m         timeout_msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. The task \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m has been ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(future)\n\u001b[0;32m    360\u001b[0m     )\n\u001b[1;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\_private\\auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     23\u001b[0m     auto_init_ray()\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\_private\\worker.py:2524\u001b[0m, in \u001b[0;36mget\u001b[1;34m(object_refs, timeout)\u001b[0m\n\u001b[0;32m   2522\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[0;32m   2523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[1;32m-> 2524\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[0;32m   2525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[1;31mRayTaskError(RuntimeError)\u001b[0m: \u001b[36mray::EvaluationActor.call()\u001b[39m (pid=19748, ip=127.0.0.1, actor_id=d5880e87f71e3599ee0a9bc201000000, repr=<evotorch.core.EvaluationActor object at 0x0000025037736A90>)\n  File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n  File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n    return method(__ray_actor, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n    return method(self, *_args, **_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 185, in call\n    return getattr(self._problem, method_name)(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 3074, in _sample_and_compute_gradients\n    resulting_batch = sample_evaluated_batch()\n                      ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 3067, in sample_evaluated_batch\n    self.evaluate(batch)\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2395, in evaluate\n    self._evaluate_all(batch)\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2413, in _evaluate_all\n    self._evaluate_batch(batch)\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 345, in _evaluate_batch\n    return super()._evaluate_batch(batch)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2447, in _evaluate_batch\n    self._evaluate(sln)\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\neproblem.py\", line 424, in _evaluate\n    fitnesses = evaluator(self.parameterize_net(parameters))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 338, in _evaluate_network\n    loss += self._evaluate_using_minibatch(network, self._current_minibatch) / self._num_minibatches\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 264, in _evaluate_using_minibatch\n    yhat = network(x)\n           ^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_1180\\556631100.py\", line 67, in forward\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 217, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_1180\\556631100.py\", line 25, in forward\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-03 13:54:01,584\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::EvaluationActor.call()\u001b[39m (pid=23744, ip=127.0.0.1, actor_id=d0394b735e7fb9664c9a39d501000000, repr=<evotorch.core.EvaluationActor object at 0x000001FA967E4790>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 185, in call\n",
      "    return getattr(self._problem, method_name)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 3074, in _sample_and_compute_gradients\n",
      "    resulting_batch = sample_evaluated_batch()\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 3067, in sample_evaluated_batch\n",
      "    self.evaluate(batch)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2395, in evaluate\n",
      "    self._evaluate_all(batch)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2413, in _evaluate_all\n",
      "    self._evaluate_batch(batch)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 345, in _evaluate_batch\n",
      "    return super()._evaluate_batch(batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2447, in _evaluate_batch\n",
      "    self._evaluate(sln)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\neproblem.py\", line 424, in _evaluate\n",
      "    fitnesses = evaluator(self.parameterize_net(parameters))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 338, in _evaluate_network\n",
      "    loss += self._evaluate_using_minibatch(network, self._current_minibatch) / self._num_minibatches\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 264, in _evaluate_using_minibatch\n",
      "    yhat = network(x)\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_1180\\556631100.py\", line 67, in forward\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_1180\\556631100.py\", line 25, in forward\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
      "2023-09-03 13:54:01,602\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::EvaluationActor.call()\u001b[39m (pid=20996, ip=127.0.0.1, actor_id=2f33d78c35457df387ab265401000000, repr=<evotorch.core.EvaluationActor object at 0x000001ED35614890>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 185, in call\n",
      "    return getattr(self._problem, method_name)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 3074, in _sample_and_compute_gradients\n",
      "    resulting_batch = sample_evaluated_batch()\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 3067, in sample_evaluated_batch\n",
      "    self.evaluate(batch)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2395, in evaluate\n",
      "    self._evaluate_all(batch)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2413, in _evaluate_all\n",
      "    self._evaluate_batch(batch)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 345, in _evaluate_batch\n",
      "    return super()._evaluate_batch(batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2447, in _evaluate_batch\n",
      "    self._evaluate(sln)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\neproblem.py\", line 424, in _evaluate\n",
      "    fitnesses = evaluator(self.parameterize_net(parameters))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 338, in _evaluate_network\n",
      "    loss += self._evaluate_using_minibatch(network, self._current_minibatch) / self._num_minibatches\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 264, in _evaluate_using_minibatch\n",
      "    yhat = network(x)\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_1180\\556631100.py\", line 67, in forward\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_1180\\556631100.py\", line 25, in forward\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
      "2023-09-03 13:54:01,606\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::EvaluationActor.call()\u001b[39m (pid=19748, ip=127.0.0.1, actor_id=d5880e87f71e3599ee0a9bc201000000, repr=<evotorch.core.EvaluationActor object at 0x0000025037736A90>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 185, in call\n",
      "    return getattr(self._problem, method_name)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 3074, in _sample_and_compute_gradients\n",
      "    resulting_batch = sample_evaluated_batch()\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 3067, in sample_evaluated_batch\n",
      "    self.evaluate(batch)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2395, in evaluate\n",
      "    self._evaluate_all(batch)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2413, in _evaluate_all\n",
      "    self._evaluate_batch(batch)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 345, in _evaluate_batch\n",
      "    return super()._evaluate_batch(batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2447, in _evaluate_batch\n",
      "    self._evaluate(sln)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\neproblem.py\", line 424, in _evaluate\n",
      "    fitnesses = evaluator(self.parameterize_net(parameters))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 338, in _evaluate_network\n",
      "    loss += self._evaluate_using_minibatch(network, self._current_minibatch) / self._num_minibatches\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 264, in _evaluate_using_minibatch\n",
      "    yhat = network(x)\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_1180\\556631100.py\", line 67, in forward\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_1180\\556631100.py\", line 25, in forward\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n",
      "2023-09-03 13:54:01,609\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::EvaluationActor.call()\u001b[39m (pid=27088, ip=127.0.0.1, actor_id=7b4af88cc7c24168a646cde901000000, repr=<evotorch.core.EvaluationActor object at 0x000001F407FC4590>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1424, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 1364, in ray._raylet.execute_task.function_executor\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\_private\\function_manager.py\", line 726, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 464, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 185, in call\n",
      "    return getattr(self._problem, method_name)(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 3074, in _sample_and_compute_gradients\n",
      "    resulting_batch = sample_evaluated_batch()\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 3067, in sample_evaluated_batch\n",
      "    self.evaluate(batch)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2395, in evaluate\n",
      "    self._evaluate_all(batch)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2413, in _evaluate_all\n",
      "    self._evaluate_batch(batch)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 345, in _evaluate_batch\n",
      "    return super()._evaluate_batch(batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\core.py\", line 2447, in _evaluate_batch\n",
      "    self._evaluate(sln)\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\neproblem.py\", line 424, in _evaluate\n",
      "    fitnesses = evaluator(self.parameterize_net(parameters))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 338, in _evaluate_network\n",
      "    loss += self._evaluate_using_minibatch(network, self._current_minibatch) / self._num_minibatches\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\evotorch\\neuroevolution\\supervisedne.py\", line 264, in _evaluate_using_minibatch\n",
      "    yhat = network(x)\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_1180\\556631100.py\", line 67, in forward\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_1180\\556631100.py\", line 25, in forward\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n"
     ]
    }
   ],
   "source": [
    "searcher.run(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f611654",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'evotorch' has no attribute 'Network'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m networks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     network \u001b[38;5;241m=\u001b[39m \u001b[43mevotorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNetwork\u001b[49m()\n\u001b[0;32m      7\u001b[0m     networks\u001b[38;5;241m.\u001b[39mappend(network)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Evaluate the networks.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'evotorch' has no attribute 'Network'"
     ]
    }
   ],
   "source": [
    "import evotorch\n",
    "\n",
    "# Create a list of networks.\n",
    "networks = []\n",
    "for i in range(100):\n",
    "    network = evotorch.Network()\n",
    "    networks.append(network)\n",
    "\n",
    "# Evaluate the networks.\n",
    "for network in networks:\n",
    "    network.evaluate()\n",
    "\n",
    "# Select the best networks.\n",
    "best_networks = networks.select_best(10)\n",
    "\n",
    "# Recombine the best networks.\n",
    "new_networks = []\n",
    "for i in range(len(best_networks)):\n",
    "    new_networks.append(best_networks[i].recombine())\n",
    "\n",
    "# Mutate the best networks.\n",
    "for network in new_networks:\n",
    "    network.mutate()\n",
    "\n",
    "# Return the new list of networks.\n",
    "return new_networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4281900-d8aa-4ffd-b3df-6d7353d603a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[47,50]\\n',\n",
       " '10\\n',\n",
       " '1\\n',\n",
       " 'relu_all\\n',\n",
       " '[0.5, 0.5]\\n',\n",
       " '16\\n',\n",
       " '19\\n',\n",
       " '0.0008724419871305545\\n',\n",
       " 'sgd\\n',\n",
       " 'mse\\n',\n",
       " '20\\n',\n",
       " '10\\n',\n",
       " 'rgrss']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"cnn.txt\",\"r\") as f:\n",
    "    text=f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40dc5c6a-ec8e-47a5-be34-9da50a5dcbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rgrss'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=open(\"cnn.txt\",\"r\")\n",
    "hidden_fc_layers_str=transform_format(file.readline())\n",
    "hidden_fc_layer=[]\n",
    "for i in hidden_fc_layers_str:\n",
    "    hidden_fc_layer.append(int(i))\n",
    "hidden_fc_layer\n",
    "input_dim=int(file.readline())\n",
    "output_dim=int(file.readline())\n",
    "activation_function=transform_format(file.readline())\n",
    "dropout=transform_format(file.readline())\n",
    "dropout_lst=[]\n",
    "for i in dropout:\n",
    "    dropout_lst.append(i)\n",
    "batch_size=int(file.readline())\n",
    "epochs=int(file.readline())\n",
    "learning_rate=float(file.readline())\n",
    "optim_ref_str=transform_format(file.readline())\n",
    "criter_ref=transform_format(file.readline())\n",
    "print_every=int(file.readline())\n",
    "patience=int(file.readline())\n",
    "mode=transform_format(file.readline())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "257dfb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_format(string):\n",
    "\t\tstring = string.replace(\"[\",\"\")\n",
    "\t\tstring = string.replace(\"]\",\"\")\n",
    "\t\tstring = string.replace(\" \",\"\")\n",
    "\t\tstring = string.replace(\"\\n\",\"\")\n",
    "\t\tstring = string.replace(\"\\\"\",\"\")\n",
    "\t\tstring = string.replace(\"'\",\"\")\n",
    "\t\tif \",\" in string or ',' in string:\n",
    "\t\t\tstring = string.split(\",\")\n",
    "\t\treturn string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d731a77-efad-41f1-8ad1-fd32bfdc0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ad43396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([16, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "input = torch.randn(16, 10, requires_grad=True)\n",
    "target = torch.randn(10)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a733cd6-cc6b-4218-beab-ef89e6eb5d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.view(10,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "464f0f6b-90b3-45f1-83c0-2cfe6d25bb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 16), (16, 32), (32, 64), (64, 128)]\n"
     ]
    }
   ],
   "source": [
    "layers = [1, 16, 32, 64, 128]\n",
    "\n",
    "pairwise_layers = lambda layers: list(zip(layers, layers[1:]))\n",
    "\n",
    "result = pairwise_layers(layers)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fb9f070-b237-4e21-aa06-61c2bbcc61c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 16), (16, 32), (32, 64), (64, 128)]\n"
     ]
    }
   ],
   "source": [
    "layers = [1, 16, 32, 64, 128]\n",
    "\n",
    "pairwise_layers = lambda layers: list(zip(layers, layers[1:]))\n",
    "\n",
    "result = pairwise_layers(layers)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbffc86b-64cc-445e-b821-375cb3a0e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b593d4ac-a3f1-490a-bb15-0d63edbed2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,copy,time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "criterion_funcs = {\"nlll\":nn.NLLLoss, \"mse\":nn.MSELoss, \"crossentl\":nn.CrossEntropyLoss, \"bcewll\":nn.BCEWithLogitsLoss}\n",
    "activation_funcs = {\"relu\":F.relu, \"sigmoid\":F.sigmoid}#, F.log_softmax, F.softmax}\n",
    "optimization_funcs = {\"adam\":optim.Adam, \"sgd\":optim.SGD}\n",
    "\n",
    "# ==== BEGIN Descriptors ====\n",
    "class Network_Descriptor: # In this class we define all the elements than conform the descriptor of a Neural Network\n",
    "\t# This attributes are common to all the neural network descriptors, no matter their type\n",
    "\t# === BEGIN Attributes ===\n",
    "\t# * hidden_fc_layers = contains the list with the number of neurons for each hidden fully connected layer\n",
    "\t# * input_dim = contains the number of input neurons of the very first layer (the number of features of each instance)\n",
    "\t# * output_dim = contains the number of output neurons of the very last layer (the number of classes to classify)\n",
    "\t# * act_functions = contains the activation functions\n",
    "\t# * batch_size = contains the size of the batches\n",
    "\t# * dropout = contains the number corresponding to the dropout of the network\n",
    "\t# * epochs = contains the number of times the entire training set is trained\n",
    "\t# * learning_rate = contains the learning rate of the network\n",
    "\t# * optim_ref = contains the reference to the optimization function\n",
    " \t# * criter_ref = contains the reference to the criterion\n",
    " \t# * print_every = defines how often the running loss will be printed\n",
    " \t# * patience = defines how many times in a row the current running loss can be greater than the previous running loss\n",
    "\t# === END Attributes ===\n",
    "\tdef __init__(self, hidden_fc_layers, input_dim, output_dim, act_funcs_ref, dropout, \\\n",
    "\t\tbatch_size, epochs, learning_rate, optim_ref, criter_ref, print_every, patience):\n",
    "\t\t\n",
    "\t\tself.hidden_fc_layers = hidden_fc_layers\n",
    "\t\tself.input_dim = input_dim\n",
    "\t\tself.output_dim = output_dim\n",
    "\t\tself.act_functions_ref = act_funcs_ref\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.epochs = epochs\n",
    "\t\tself.optim_ref = optim_ref\n",
    "\t\tself.criter_ref = criter_ref\n",
    "\t\tself.print_every = print_every\n",
    "\t\tself.patience = patience\n",
    "\n",
    "\tdef change_hidden_fc_layers(self, hidden_fc_layers):\n",
    "\t\tself.hidden_fc_layers = hidden_fc_layers\n",
    "\tdef change_input_dim(self,input_dim):\n",
    "\t\tself.input_dim = input_dim\n",
    "\tdef change_output_dim(self,output_dim):\n",
    "\t\tself.output_dim = output_dim\n",
    "\tdef change_act_functions_ref(self,act_functions_ref):\n",
    "\t\tself.act_functions_ref = act_functions_ref\n",
    "\tdef change_batch_size(self,batch_size):\n",
    "\t\tself.batch_size = batch_size\n",
    "\tdef change_dropout(self,dropout):\n",
    "\t\tself.dropout = dropout\n",
    "\tdef change_learning_rate(self, learning_rate):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\tdef change_epochs(self, epochs):\n",
    "\t\tself.epochs = epochs\n",
    "\tdef change_optim_ref(self, change_optim_ref):\n",
    "\t\tself.optim_ref = change_optim_ref\n",
    "\tdef change_criter_ref(self, change_criter_ref):\n",
    "\t\tself.criter_ref = change_criter_ref\n",
    "\tdef change_print_every(self, print_every):\n",
    "\t\tself.print_every = print_every\n",
    "\tdef change_patience(self, patience):\n",
    "\t\tself.patience = patience\n",
    "\n",
    "\t# Transforms the data from the txt-native format to Evotorch format\n",
    "\tdef transform_format(self, string):\n",
    "\t\tstring = string.replace(\"[\",\"\")\n",
    "\t\tstring = string.replace(\"]\",\"\")\n",
    "\t\tstring = string.replace(\" \",\"\")\n",
    "\t\tstring = string.replace(\"\\n\",\"\")\n",
    "\t\tstring = string.replace(\"\\\"\",\"\")\n",
    "\t\tstring = string.replace(\"'\",\"\")\n",
    "\t\tif \",\" in string or ',' in string:\n",
    "\t\t\tstring = string.split(\",\")\n",
    "\t\treturn string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8944c858-2535-45e7-9bb3-e1aa641b9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Descriptor(Network_Descriptor):\n",
    "\t# === BEGIN Attributes ===\n",
    "\t# * conv_layers = contains the list with the number of channels for each convolutional layer\n",
    "\t# * kernel_sizes = defines the sizes of the kernels. It must have these shape: [ [x,y], [x,y],...,[x,y] ]\n",
    "\t# where x defines the kernel size of the i-th convolutional layer and y defines the kernel size of the i-th pooling layer\n",
    "\t# * conv_stride_sizes = defines the sizes of the strides of the convolutional layer. It must have these shape: [x,x,x,...]\n",
    "\t# where x defines the stride size of the i-th convolutional layer\n",
    "\t# === END Attributes ===\n",
    "\tdef __init__(self, hidden_fc_layers = [5,5], input_dim = 50, output_dim = 1, \\\n",
    "\t\tactivation_funcs_ref = \"relu_all\", dropout = [0.5,0.5], batch_size = 1, epochs = 1, learning_rate = 0.1,\\\n",
    "\t\toptim_ref = \"adam\", criter_ref =  \"nlll\", print_every = 40, patience = 5, conv_layers = [1,32,64], \\\n",
    "\t\tkernel_sizes = [ [5,2],[5,2] ], conv_stride_sizes = [1,1]):\t\n",
    "\t\t\n",
    "\t\tsuper().__init__(hidden_fc_layers, input_dim, output_dim, activation_funcs_ref, \\\n",
    "\t\t\tdropout, batch_size, epochs, learning_rate, optim_ref, criter_ref, print_every, patience)\n",
    "\t\t\n",
    "\t\tself.conv_layers = conv_layers # [ number of filters on each convolution layer ]\n",
    "\t\tself.kernel_sizes = kernel_sizes # [ [kernel size of convolution layer, kernel size of pooling layer] ]\n",
    "\t\tself.conv_stride_sizes = conv_stride_sizes # [ [stride size of convolution layer, stride size of pooling layer] ]\n",
    "\n",
    "\tdef change_conv_layers(self, conv_layers):\n",
    "\t\tself.conv_layers = conv_layers\n",
    "\tdef change_kernel_sizes(self,kernel_sizes):\n",
    "\t\tself.kernel_sizes = kernel_sizes\n",
    "\tdef change_conv_stride_sizes(self, conv_stride_sizes):\n",
    "\t\tself.conv_stride_sizes = conv_stride_sizes\n",
    "\n",
    "\n",
    "\t# We save the NN's information on a text file we passed in file_name\n",
    "\tdef save_NN_info(self, file_name):\n",
    "\t\tf = open(file_name, \"w\")\n",
    "\t\tinfo = str(self.hidden_fc_layers)+\"\\n\"+str(self.input_dim)+\"\\n\"+str(self.output_dim)+\"\\n\"+\\\n",
    "\t\tstr(self.act_functions_ref)+\"\\n\"+str(self.dropout)+\"\\n\"+str(self.batch_size)+\"\\n\"+str(self.epochs)+\"\\n\"+str(self.learning_rate)+\"\\n\"+\\\n",
    "\t\tstr(self.optim_ref)+\"\\n\"+str(self.criter_ref)+\"\\n\"+str(self.print_every)+\"\\n\"+str(self.patience)+\"\\n\"+\\\n",
    "\t\tstr(self.conv_layers)+\"\\n\"+str(self.kernel_sizes)+\"\\n\"+str(self.conv_stride_sizes)\n",
    "\t\tf.write(info)\n",
    "\t\tf.close()\n",
    "\n",
    "\n",
    "\t# We load the NN's information from a text file we passed in file_name\n",
    "\tdef load_NN_info(self, file_name):\n",
    "\t\tf = open(file_name, \"r\")\n",
    "\t\thidden_fc_layers_str = self.transform_format(f.readline())\n",
    "\t\tself.hidden_fc_layers = []\n",
    "\t\tif type(hidden_fc_layers_str) == list:\n",
    "\t\t\tfor i in hidden_fc_layers_str:\n",
    "\t\t\t\tself.hidden_fc_layers.append(int(i))\n",
    "\t\telse:\n",
    "\t\t\tself.hidden_fc_layers.append(int(hidden_fc_layers_str))\n",
    "\n",
    "\t\tself.input_dim = int(f.readline())\n",
    "\t\tself.output_dim = int(f.readline())\n",
    "\t\t\n",
    "\t\tact_functions_refs_str = self.transform_format(f.readline())\n",
    "\t\tif act_functions_refs_str == \"relu_all\":\n",
    "\t\t\tself.act_functions_ref = \"relu_all\"\n",
    "\t\telif act_functions_refs_str == \"sigmoid_all\":\n",
    "\t\t\tself.act_functions_ref = \"sigmoid_all\"\n",
    "\t\telse:\n",
    "\t\t\tself.act_functions_ref = []\n",
    "\t\t\tif type(act_functions_refs_str) == list:\n",
    "\t\t\t\tfor i in act_functions_refs_str:\n",
    "\t\t\t\t\tself.act_functions_ref.append(i)\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.act_functions_ref.append(act_functions_refs_str)\n",
    "\t\tdropout_refs_str = self.transform_format(f.readline())\n",
    "\t\tself.dropout = []\n",
    "\t\tif type(dropout_refs_str) == list:\n",
    "\t\t\tfor i in dropout_refs_str:\n",
    "\t\t\t\tself.dropout.append(float(i))\n",
    "\t\telse:\n",
    "\t\t\tself.dropout.append(float(dropout_refs_str))\n",
    "\n",
    "\t\tself.batch_size = int(f.readline())\n",
    "\t\tself.epochs = int(f.readline())\n",
    "\t\tself.learning_rate = float(f.readline())\n",
    "\n",
    "\t\toptim_ref_str = self.transform_format(f.readline())\n",
    "\t\tself.optim_ref = optim_ref_str\n",
    "\n",
    "\t\tcriter_ref_str = self.transform_format(f.readline())\n",
    "\t\tself.criter_ref = criter_ref_str\n",
    "\n",
    "\t\tself.print_every = int(f.readline())\n",
    "\t\tself.patience = int(f.readline())\n",
    "\n",
    "\n",
    "\t\tself.conv_layers = []\n",
    "\t\tconv_layers_str = self.transform_format(f.readline())\n",
    "\t\tfor i in conv_layers_str:\n",
    "\t\t\tself.conv_layers.append(int(i))\n",
    "\n",
    "\t\tself.kernel_sizes = []\n",
    "\t\tkernel_sizes_str = self.transform_format(f.readline())\n",
    "\t\t# As it is a list with this form [ [x,y], [x,y],..., [x,y]] we use the zipping\n",
    "\t\tkernel_size_pairs = zip(kernel_sizes_str[0::2], kernel_sizes_str[1::2])\n",
    "\n",
    "\t\tself.kernel_sizes  = [ [int(i),int(j)] for i,j in kernel_size_pairs]\n",
    "\n",
    "\t\tself.conv_stride_sizes = []\n",
    "\n",
    "\t\tstride_sizes_str = self.transform_format(f.readline())\n",
    "\n",
    "\t\tfor i in stride_sizes_str:\n",
    "\t\t\tself.conv_stride_sizes.append(int(i))\n",
    "# ==== END Descriptors ====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "790f8dd9-e214-4bd8-ae22-b3d3c0ed0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\t# This attributes are common to all the neural networks, no matter their type\n",
    "\t# === BEGIN Attributes ===\n",
    "\t# * descriptor = contains the network descriptor\n",
    "\t# * dropout = contains the dropout layer of the network\n",
    "\t# * act_functions = contains the activation functions\n",
    "\t# * criterion = contains the loss function of the network\n",
    "\t# === END Attributes ===\n",
    "\n",
    "\tdef __init__(self,Network_Descriptor):\n",
    "\t\t\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\ttorch.set_default_dtype(torch.float64)\n",
    "\n",
    "\t\tself.descriptor = Network_Descriptor\n",
    "\t\tself.dropout = nn.ModuleList([nn.Dropout(p = i) for i in self.descriptor.dropout]) # the length has to be == len(hidden_fc_layers)\n",
    "\n",
    "\t\tself.act_functions = []\n",
    "\n",
    "\t\tif self.descriptor.act_functions_ref == \"relu_all\":\n",
    "\t\t\tfor i in range(len(self.descriptor.hidden_fc_layers)):\n",
    "\t\t\t\tself.act_functions.append(F.relu)\n",
    "\t\telif self.descriptor.act_functions_ref == \"sigmoid_all\":\n",
    "\t\t\tfor i in range(len(self.descriptor.hidden_fc_layers)):\n",
    "\t\t\t\tself.act_functions.append(F.sigmoid)\n",
    "\t\telse:\n",
    "\t\t\tfor i in self.descriptor.act_functions_ref: # the length of self.descriptor.act_functions_ref has to be == len(hidden_fc_layers)\n",
    "\t\t\t\tself.act_functions.append(activation_funcs[i])\n",
    "\n",
    "\t\tself.criterion = criterion_funcs[self.descriptor.criter_ref]()#reduction='sum', size_average=False)\n",
    "\n",
    "\tdef predict(self, x):\n",
    "\t\treturn self(x)\n",
    "\n",
    "# Saves the values of the NN's hiperparameters in the file. In other words, it saves the values of the NN's descriptor\n",
    "\tdef save_NN_info(self, file_name): \n",
    "\t\tself.descriptor.save_NN_info(file_name)\n",
    "\n",
    "# Saves the NN with all the weights, biases and parameters\n",
    "\tdef save_NN(self, file_name): \n",
    "\t\ttorch.save(self.state_dict(), file_name)\n",
    "\n",
    "# Loads the NN with all the weights, biases and parameters\n",
    "\tdef load_NN(self, file_name, strictt = False):  \n",
    "\t\tself.load_state_dict(torch.load(file_name), strict=strictt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79e46f43-c0ac-44bc-a036-1a6e24cbf44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Network(Network):\n",
    "\t# === BEGIN Attributes ===\n",
    "\t# * conv_layers = contains the sequences of (convolutional layers + relu layers + MaxPooling layers)\n",
    "\t# * _to_linear = contains the number of neurons that the hidden fc layer connected to the las conv_layer has\n",
    "\t# * hidden_fc_layers = contains the hidden fully connected layers\n",
    "\t# * output = contains the last layer of the network, the one which computes the prediction\n",
    "\t# * optimizer = contains the optimization algorithm's instance\n",
    "\t# === END Attributes ===\n",
    "\tdef __init__(self,network_descriptor):\n",
    "\n",
    "\t\tsuper().__init__(network_descriptor)\n",
    "\n",
    "\t\tlayer_sizes = zip(self.descriptor.conv_layers[:-1], self.descriptor.conv_layers[1:])\n",
    "\t\t# When we define a convolutional layer what we are really defining is a sequence of three layers:\n",
    "\t\t# Convolutional layer + Relu layer + MaxPooling layer\n",
    "\t\t\n",
    "\t\tself.conv_layers = nn.ModuleList([nn.Sequential(\n",
    "\t\t\tnn.Conv2d(h1, h2, kernel_size = self.descriptor.kernel_sizes[i][0], stride = self.descriptor.conv_stride_sizes[i], \\\n",
    "\t\t\t\tpadding = 0),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(kernel_size = self.descriptor.kernel_sizes[i][1])) for i,(h1, h2) in enumerate(layer_sizes) ] )\n",
    "\n",
    "\t\t# # This random value is created in order to get the self._to_linear value, which is very useful\n",
    "\t\tx = torch.randn(self.descriptor.input_dim, self.descriptor.input_dim).view(-1,1, \\\n",
    "\t\t\tself.descriptor.input_dim,self.descriptor.input_dim)\n",
    "\t\t# This variable transforms the data we have passed through the convolution function so that we can\n",
    "\t\t# forward-pass it\n",
    "\t\tself._to_linear = None\n",
    "\t\t# # Here we get the value of self._to_linear, which we will use in order to shape the size of the data correctly\n",
    "\t\ttry:\n",
    "\t\t\tself.convs(x)\n",
    "\t\t\t# a = self.convs_sizes(x.shape[3])\n",
    "\t\t\t# print(r.shape[3], a)\n",
    "\t\t\t# size_is_1 = False\n",
    "\t\t\t# numb_of_convl = len(self.descriptor.kernel_sizes)\n",
    "\t\t\t# i = 0\n",
    "\t\t\t# _size_ = list(x.size())[3]\n",
    "\t\t\t# while not size_is_1 and i < numb_of_convl:\n",
    "\t\t\t# \t_size_ = self.get_tensor_sz_after_convpool(list(x.size())[3])\n",
    "\t\t\t# \tprint(i, numb_of_convl)\n",
    "\t\t\t# \tif _size_ <= 1 or self.descriptor.kernel_sizes[i][0]>=_size_ or self.descriptor.kernel_sizes[i][1]>=_size_:\n",
    "\t\t\t# \t\tsize_is_1 = True\n",
    "\t\t\t# \t\tself.conv_layers = self.conv_layers[:i+1]\n",
    "\t\t\t# \t\tself.descriptor.conv_layers = self.descriptor.conv_layers[:i+1]\n",
    "\t\t\t# \t\tself.descriptor.kernel_sizes = self.descriptor.kernel_sizes[:i]\n",
    "\t\t\t# \t\tself.descriptor.conv_stride_sizes = self.descriptor.conv_stride_sizes[:i]\n",
    "\t\t\t# \t\tnumb_of_convl = len(self.descriptor.conv_layers)\n",
    "\t\t\t# \telse:\n",
    "\t\t\t# \t\ti+=1\n",
    "\t\t\t# self._to_linear = x.shape[1]*x.shape[2]*self.convs_sizes(list(x.size())[3])\n",
    "\t\texcept Exception as e:\n",
    "\t\t\traise Exception(\"Convolution failed!\")\n",
    "\n",
    "\t\t# Above, the process is the same as in MLP_Network and fills the same purpose\n",
    "\t\tself.hidden_fc_layers = nn.ModuleList([nn.Linear(self._to_linear, self.descriptor.hidden_fc_layers[0])])\n",
    "\n",
    "\t\tlayer_sizes = zip( self.descriptor.hidden_fc_layers[:-1], self.descriptor.hidden_fc_layers[1:] )\n",
    "\n",
    "\t\tself.hidden_fc_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "\n",
    "\t\tself.output = nn.Linear(self.descriptor.hidden_fc_layers[-1], self.descriptor.output_dim)\n",
    "\t\t# We initialize the optimizer\n",
    "\t\tself.optimizer = optimization_funcs[self.descriptor.optim_ref](self.parameters(), \\\n",
    "\t\t\tlr = self.descriptor.learning_rate)#, weight_decay=0.01)\n",
    "\t\t# print(\"CNN initialization successful\")\n",
    "\n",
    "# Here we load the hiperparameters from a file we passed as an argument (file_name). \n",
    "# file_name also includes the path\n",
    "\tdef load_NN_info(self, file_name): \n",
    "\t\tself.descriptor.load_NN_info(file_name)\n",
    "\t\tself.__init__(self.descriptor)\n",
    "\n",
    "\tdef get_conv_output_size(self, x_size, conv_kernel_size, conv_stride_size):\n",
    "\t\t# ((W1F  +2P)/S)  +1\n",
    "\t\t# [(I  F +2 *P) / S] +1 x D | I: input , F: filter size, P: pooling, D: number of feature maps\n",
    "\t\t# P == pooling == 0\n",
    "\t\treturn ((x_size-conv_kernel_size)/conv_stride_size)+1 \n",
    "\n",
    "\tdef get_pool_output_size(self, x_size, pool_kernel_size):\n",
    "\t\t# (W1F)/S  +1\n",
    "\t\t# # [(I  F) / S] + 1 x D | I: input , F: filter size, D: number of feature maps\n",
    "\t\t# pool_kernel_size == pool_stride_size\t\n",
    "\t\treturn ((x_size-pool_kernel_size)/pool_kernel_size)+1\n",
    "\n",
    "\tdef get_convpool_size(self, x_size, conv_kernel_size, conv_stride_size, pool_kernel_size):\n",
    "\t\tx_sz = self.get_conv_output_size(x_size, conv_kernel_size, conv_stride_size)\n",
    "\t\tx_sz = self.get_pool_output_size(x_sz, pool_kernel_size)\n",
    "\t\treturn x_sz\n",
    "\n",
    "\n",
    "\tdef get_tensor_sz_after_convpool(self, x_size):\n",
    "\t\tfor i in range(len(self.descriptor.kernel_sizes)):\n",
    "\t\t\tx_size = int(self.get_convpool_size(x_size, self.descriptor.kernel_sizes[i][0],\\\n",
    "\t\t\t\tself.descriptor.conv_stride_sizes[i], self.descriptor.kernel_sizes[i][1]))\n",
    "\t\treturn x_size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tdef convs_sizes(self, x):\n",
    "\t\toriginal_x = copy.deepcopy(x)\n",
    "\t\t# print(\"Estos son los conv layers \", self.conv_layers)\n",
    "\t\tfor i,conv in enumerate(self.conv_layers):\n",
    "\t\t\tx = self.get_tensor_sz_after_convpool(x)\n",
    "\t\t\t# print( \"Size of the convolution \", self.get_conv_output_size(x) ,list(x.size()))\n",
    "\t\t\tif x <= 1 or self.descriptor.kernel_sizes[i][0]>=x or self.descriptor.kernel_sizes[i][1]>=x:\n",
    "\t\t\t\t# print(\"Da tamaino 1 con conv_layers \", self.descriptor.conv_layers)\n",
    "\t\t\t\tself.conv_layers = self.conv_layers[:-1]\n",
    "\t\t\t\tself.descriptor.conv_layers = self.descriptor.conv_layers[:-1]\n",
    "\t\t\t\tself.descriptor.kernel_sizes = self.descriptor.kernel_sizes[:-1]\n",
    "\t\t\t\tself.descriptor.conv_stride_sizes = self.descriptor.conv_stride_sizes[:-1]\n",
    "\t\t\t\tx = self.convs_sizes(original_x)\n",
    "\t\t\t\tbreak\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "\n",
    "# In this function the convolution and the pooling take place\n",
    "\tdef convs(self, x):\n",
    "\t\toriginal_x = copy.deepcopy(x)\n",
    "\t\tfor i,conv in enumerate(self.conv_layers):\n",
    "\t\t\tx = conv(x)\n",
    "\t\t\t_size_ = list(x.size())[3]\n",
    "\t\t\tif _size_ <= 1 or self.descriptor.kernel_sizes[i][0]>=_size_ or self.descriptor.kernel_sizes[i][1]>=_size_:\n",
    "\t\t\t\t# print(\"Da tamaino 1 con conv_layers \", self.descriptor.conv_layers)\n",
    "\t\t\t\tself.conv_layers = self.conv_layers[:-1]\n",
    "\t\t\t\tself.descriptor.conv_layers = self.descriptor.conv_layers[:-1]\n",
    "\t\t\t\tself.descriptor.kernel_sizes = self.descriptor.kernel_sizes[:-1]\n",
    "\t\t\t\tself.descriptor.conv_stride_sizes = self.descriptor.conv_stride_sizes[:-1]\n",
    "\t\t\t\tx = self.convs(original_x)\n",
    "\t\t\t\tbreak\n",
    "\t\tif self._to_linear is None:\n",
    "\t\t\tself._to_linear = x.shape[1]*x.shape[2]*x.shape[3]\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "# Note that tensor.shape is an alias to tensor.size(), though tensor.shape is an\n",
    "# attribute of the tensor in question whereas tensor.size() is a function.\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# We make the convolution\n",
    "\t\tx = self.convs(x)\n",
    "\t\tx = x.reshape(x.size(0), -1) # This line transforms x's dimensionality into: [batch_size, self._to_linear]\n",
    "\t\t# # We put the data on its corresponding format (I THINK IT IS DEPRECATED)\n",
    "\t\t# x = x.view(-1,1, self._to_linear)\n",
    "\t\t# We do the forward pass\n",
    "\t\tfor linear in range(len(self.hidden_fc_layers)):\n",
    "\t\t\tx = self.act_functions[linear](self.hidden_fc_layers[linear](x))\n",
    "\t\t\tx = self.dropout[linear](x)\n",
    "\t\tx = self.output(x) # This is the output layer, so we dont apply the actvation function\t\n",
    "\t\treturn F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\tdef training_CNN(self,training_data):\n",
    "\t\t\n",
    "\t\tsteps = 0 \n",
    "\t\trunning_loss = 0.0\n",
    "\t\tpatience_count = 0\n",
    "\t\thalt = False\n",
    "\t\tlast_loss = float(\"Inf\")\n",
    "\t\trunning_losses = []\n",
    "\t\tepochs = []\n",
    "\t\tself.optimizer.zero_grad()\n",
    "\n",
    "\t\tprint(\"======== TRAINING PHASE ========\\n\\n\")\n",
    "\n",
    "\t\tfor e in range(self.descriptor.epochs):\n",
    "\t\t\t\n",
    "\t\t\tprint(\"EPOCH \", e)\n",
    "\n",
    "\t\t\tfor batch in training_data:\n",
    "\t\t\t\tvalues, labels = batch\n",
    "\t\t\t\t# We will have to delete the following two lines\n",
    "\t\t\t\t# values = values.double()\n",
    "\t\t\t\t# labels = labels.long()\n",
    "\n",
    "\t\t\t\tvalues = values.view(-1, 1, self.descriptor.input_dim, self.descriptor.input_dim)\n",
    "\t\t\t\t# We put the gradients to zero\n",
    "\t\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\t\t# We update the number of steps\t\t\n",
    "\t\t\t\tsteps+=1\n",
    "\n",
    "\t\t\t\toutput = self.predict(values.double())#output = self(values.double())\n",
    "\t\t\t\t# We transform the dimensions of the output\n",
    "\t\t\t\toutput = output.view(-1,self.descriptor.output_dim)\n",
    "\t\t\t\t# We compute the loss\n",
    "\t\t\t\tif list(output.size())[0] != list(labels.size())[0]:\n",
    "\t\t\t\t\toutput = output.view(self.descriptor.batch_size, -1)\n",
    "                \n",
    "\t\t\t\tloss = self.criterion(output, labels)\n",
    "\t\t\t\t# We do the backpropagation\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\t# We apply the changes\n",
    "\t\t\t\tself.optimizer.step()\n",
    "\t\t\t\t# We obtain the running loss (the actual loss)\n",
    "\t\t\t\trunning_loss = loss.item() # This is the loss of the current batch, with which we are going to compute the overfitting\n",
    "\n",
    "\t\t\t\t# If the running loss has kept rising in self.descriptor.patience number of steps then\n",
    "\t\t\t\t# we halt the training\n",
    "\t\t\t\tif running_loss > last_loss or running_loss == last_loss:\n",
    "\t\t\t\t\tpatience_count += 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tpatience_count = 0.0\n",
    "\t\t\t\t# If the patience is reached, the training is halted\n",
    "\t\t\t\tif patience_count == self.descriptor.patience:\n",
    "\t\t\t\t\thalt = True\n",
    "\t\t\t\t# We keep record of the running loss for the next step\n",
    "\t\t\t\tlast_loss = copy.copy(running_loss)\n",
    "\n",
    "\t\t\t\t# The code within this is statement has the purpose of printing the running loss\n",
    "\t\t\t\t# We print the running loss\n",
    "\t\t\t\tif steps % self.descriptor.print_every == 0:\t\n",
    "\t\t\t\t\tprint(\"Running loss \", running_loss)\n",
    "\t\t\t\t# If the patience is reached, the training is halted\n",
    "\t\t\t\tif halt:\n",
    "\t\t\t\t\tbreak\t\t\t\n",
    "\t\t\t# If the patience is reached, the training is halted\n",
    "\t\t\tif halt:\n",
    "\t\t\t\tbreak\n",
    "\t\t\tepochs.append(e)\n",
    "\t\t\trunning_losses.append(running_loss)\n",
    "\t\t# if halt:\n",
    "\t\t# \tprint(\"Overfitting occurred!\")\n",
    "\t\treturn epochs, running_losses\n",
    "\t# def training_CNN2(self, testloader):\n",
    "\n",
    "\n",
    "\tdef testing_CNN(self,testloader):\n",
    "\t\t# We initialize the test loss\n",
    "\t\ttest_loss = 0\n",
    "\t\t# We initialize the number of correct predictions\n",
    "\t\tcorrect = 0\n",
    "\t\t# We initialize the number of total predictions\n",
    "\t\ttotal = 0\n",
    "\n",
    "\t\t# print(\"======== TESTING PHASE ========\\n\\n\")\n",
    "\t\tfor batch in testloader:\n",
    "\t\t# for i in tqdm(range(0, len(testing_data), self.descriptor.batch_size)): (deprecated)\n",
    "\t\t\t# We get the values and the labels\n",
    "\t\t\tvalues, labels = batch\n",
    "\t\t\t\n",
    "\t\t\t# We will have to delete the following two lines\n",
    "\t\t\t# values = values.double()\n",
    "\t\t\t# labels = labels.long()\n",
    "\t\t\tvalues = values.view(-1, 1, self.descriptor.input_dim, self.descriptor.input_dim)\n",
    "\n",
    "\t\t\toutput = self.predict(values.double())#output = self(values.double())\n",
    "\t\t\t# We redimension the output\n",
    "\t\t\toutput = output.view(-1,self.descriptor.output_dim)\n",
    "\n",
    "\t\t\tif list(output.size())[0] != list(labels.size())[0]:\n",
    "\t\t\t\toutput = output.view(self.descriptor.batch_size, -1)\n",
    "\t\t\t# We get the predictions\n",
    "\t\t\t_, predicted = torch.max(output.data, 1) # We get the index\n",
    "\t\t\t# print(\"Predicted \", predicted, \"labels \", labels)\n",
    "\t\t\t# We get the total number of instances\n",
    "\t\t\ttotal += labels.size(0)\n",
    "\t\t\t# We get the how many predictions were correct\n",
    "\t\t\tcorrect += (predicted == labels).sum().item()\n",
    "\t\t\t# print(\"Corrects: \", correct)\n",
    "\n",
    "\t\t# we get, print and return the accuracy\n",
    "\t\tresult = round(correct/total,3) # We get the accuracy\n",
    "\t\t# print(correct,\" correct out of \", total)\n",
    "\t\t# print(\"Accuracy: \", result)\n",
    "\t\treturn result\n",
    "# ==== END Implementators ====\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a885dd7-ad33-4f23-b1e8-d49bf189a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_network = CNN_Network(CNN_Descriptor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43c001ab-e6aa-482f-a887-e7fbf105673d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_Network(\n",
       "  (dropout): ModuleList(\n",
       "    (0-1): 2 x Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (criterion): NLLLoss()\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (hidden_fc_layers): ModuleList(\n",
       "    (0): Linear(in_features=5184, out_features=5, bias=True)\n",
       "    (1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (output): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c01b626f-08e5-414e-9e5c-bcde057bf789",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_info_file = \"cnn.txt\"\n",
    "n_network.load_NN_info(NN_info_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d21b8a4-90f0-4d9a-a5ef-26911f4c110e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_network.conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c4e19-4dfb-4057-97aa-148bdaca1c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
